# E-Commerce-Data-Pipeline-From-PostgreSQL-to-Hive-Using-PySpark

Ingested e-commerce datasets from PostgreSQL into Hive using PySpark, performing preliminary filtering to retain only essential data, reducing data redundancy by over 35%
Utilized Python with PySpark for a robust ETL process, efficiently extracting, cleaning, and transforming data with an emphasis on insights generation
Utilized Apache Hive for data warehousing, which leverages HDFS underneath, ensuring a robust, scalable, and fault-tolerant storage
Reloaded the transformed insights into Hive tables, utilizing SQL-like querying over Big Data storage capabilities, ensuring streamlined access for further analysis
